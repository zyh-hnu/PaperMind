# PaperMind/core/vectorstore.py

"""
é«˜æ€§èƒ½å‘é‡æ•°æ®åº“æ¨¡å— (v2.2 - FAISS Optimized)

æ ¸å¿ƒä¼˜åŒ–ç‚¹ï¼š
1.  **FAISS ä¼˜å…ˆ**: é»˜è®¤ä½¿ç”¨FAISSï¼Œå®ƒåœ¨çº¯CPUå’ŒGPUçŽ¯å¢ƒä¸‹é€šå¸¸æ¯”Chromaæ›´å¿«ã€‚
2.  **GPUä¼˜å…ˆä¸ŽåŠç²¾åº¦æµ®ç‚¹ (FP16)**: è‡ªåŠ¨æ£€æµ‹CUDA/MPSå¹¶ä½¿ç”¨FP16åŠ é€Ÿã€‚
3.  **æ™ºèƒ½æ¨¡åž‹ç®¡ç†**: é€šè¿‡å•ä¾‹æ¨¡å¼ç¼“å­˜å·²åŠ è½½çš„æ¨¡åž‹ï¼Œé¿å…é‡å¤åŠ è½½ã€‚
4.  **ä»£ç ç®€æ´**: æä¾›æ¸…æ™°ã€ç›´æŽ¥çš„å‡½æ•°æŽ¥å£ï¼Œæ˜“äºŽè°ƒç”¨å’Œç»´æŠ¤ã€‚
"""
import os
import time
import logging
from typing import List

# ç¡®ä¿åœ¨å¯¼å…¥torchä¹‹å‰è®¾ç½®çŽ¯å¢ƒå˜é‡ï¼Œä»¥é¿å…æ½œåœ¨çš„å†²çª
os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'

from langchain.schema.document import Document
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.schema.retriever import BaseRetriever
import torch

# --- å…¨å±€é…ç½®ä¸Žè®¾å¤‡æ£€æµ‹ ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def get_optimal_device() -> str:
    if torch.cuda.is_available():
        logger.info("âœ… æ£€æµ‹åˆ° CUDA è®¾å¤‡ï¼Œå°†ä½¿ç”¨ GPU åŠ é€Ÿã€‚")
        return 'cuda'
    if torch.backends.mps.is_available():
        logger.info("âœ… æ£€æµ‹åˆ° Apple Silicon (MPS) è®¾å¤‡ï¼Œå°†ä½¿ç”¨ GPU åŠ é€Ÿã€‚")
        return 'mps'
    logger.info("âš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œå°†ä½¿ç”¨ CPUã€‚")
    return 'cpu'

DEFAULT_DEVICE = get_optimal_device()

# --- æ¨¡åž‹ç®¡ç†å™¨ ---
class ModelManager:
    _instance = None
    _models_cache = {}

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def get_embedding_model(self, model_name: str, device: str) -> HuggingFaceEmbeddings:
        cache_key = f"{model_name}_{device}"
        if cache_key in self._models_cache:
            logger.info(f"ðŸš€ ä»Žç¼“å­˜ä¸­å¤ç”¨åµŒå…¥æ¨¡åž‹: {model_name} on {device}")
            return self._models_cache[cache_key]

        logger.info(f"â³ æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡åž‹: {model_name} -> è®¾å¤‡: {device}")
        start_time = time.time()
        
        model_kwargs = {'device': device}
        encode_kwargs = {'normalize_embeddings': True, 'batch_size': 128} # å¢žå¤§batch_sizeä»¥åˆ©ç”¨GPU
        
        if device != 'cpu':
            model_kwargs['torch_dtype'] = torch.float16

        embeddings = HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs=model_kwargs,
            encode_kwargs=encode_kwargs,
            show_progress=True # æ˜¾ç¤ºè¿›åº¦æ¡
        )
        self._models_cache[cache_key] = embeddings
        logger.info(f"âœ… åµŒå…¥æ¨¡åž‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f} ç§’")
        return embeddings

MODEL_MANAGER = ModelManager()

def create_vectorstore(chunks: List[str],
                       model_name: str = "BAAI/bge-base-zh-v1.5",
                       device: str = DEFAULT_DEVICE) -> FAISS:
    if not chunks:
        raise ValueError("è¾“å…¥æ–‡æœ¬å—åˆ—è¡¨ä¸èƒ½ä¸ºç©ºã€‚")

    logger.info(f"--- å¼€å§‹åˆ›å»ºFAISSå‘é‡æ•°æ®åº“ ---")
    start_time = time.time()
    
    embeddings = MODEL_MANAGER.get_embedding_model(model_name, device)
    
    # FAISS.from_texts å†…éƒ¨å·²åšé«˜æ•ˆä¼˜åŒ–
    vectorstore = FAISS.from_texts(texts=chunks, embedding=embeddings)
    
    logger.info(f"âœ… FAISSå‘é‡æ•°æ®åº“åˆ›å»ºæˆåŠŸï¼æ€»è€—æ—¶: {time.time() - start_time:.2f} ç§’")
    return vectorstore

def get_retriever(vectorstore: FAISS, top_k: int = 4) -> BaseRetriever:
    logger.info(f"åˆ›å»ºæ£€ç´¢å™¨ï¼Œå°†è¿”å›ž top {top_k} ä¸ªç›¸å…³ç»“æžœã€‚")
    return vectorstore.as_retriever(search_kwargs={"k": top_k})
