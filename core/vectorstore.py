# PaperMind/core/vectorstore.py

"""
é«˜æ€§èƒ½æ··åˆæ£€ç´¢æ¨¡å— (v3.1 - Production Grade)

æ ¸å¿ƒè®¾è®¡:
1.  **æ‹¥æŠ±LangChainç”Ÿæ€**: å®Œå…¨ä½¿ç”¨å®˜æ–¹ã€ç»è¿‡ä¼˜åŒ–çš„ BM25Retriever å’Œ EnsembleRetriever ç»„ä»¶ï¼Œç¡®ä¿æœ€ä½³æ€§èƒ½å’Œå…¼å®¹æ€§ã€‚
2.  **å…ˆè¿›çš„èåˆç®—æ³•**: åˆ©ç”¨ EnsembleRetriever å†…ç½®çš„å€’æ•°æ’åèåˆ (Reciprocal Rank Fusion, RRF) ç®—æ³•ï¼Œæ™ºèƒ½åœ°åˆå¹¶ç¨€ç–å’Œç¨ å¯†æ£€ç´¢ç»“æœï¼Œæ•ˆæœè¿œè¶…æ‰‹åŠ¨åŠ æƒã€‚
3.  **æ¶æ„è§£è€¦**: æœ¬æ¨¡å—ä¸“æ³¨äºâ€œæ£€ç´¢å™¨â€çš„æ„å»ºï¼Œå®Œå…¨ç§»é™¤äº†å¯¹LLMçš„ä¾èµ–ï¼ˆHyDEé€»è¾‘å°†ç§»è‡³chain.pyå¤„ç†ï¼‰ï¼Œéµå¾ªå…³æ³¨ç‚¹åˆ†ç¦»çš„æœ€ä½³å®è·µã€‚
4.  **ç®€æ´ä¸é«˜æ•ˆ**: ä»£ç æ›´å°‘ï¼ŒåŠŸèƒ½æ›´å¼ºï¼Œå¯ç»´æŠ¤æ€§æ›´é«˜ã€‚
"""
import os
import time
import logging
from typing import List

# ç¡®ä¿åœ¨å¯¼å…¥torchä¹‹å‰è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

from langchain.schema.document import Document
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.schema.retriever import BaseRetriever
import torch

# --- å…¨å±€é…ç½®ä¸æ—¥å¿— ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

import asyncio
from concurrent.futures import ThreadPoolExecutor

def retrieve_in_parallel(retrievers, query):
    def _invoke(r):
        return r.invoke(query)
    
    with ThreadPoolExecutor() as executor:
        futures = [executor.submit(_invoke, r) for r in retrievers]
        results = [f.result() for f in futures]
    return results

# --- è®¾å¤‡è‡ªåŠ¨æ£€æµ‹ ---
def get_optimal_device() -> str:
    """è‡ªåŠ¨æ£€æµ‹å¹¶è¿”å›æœ€ä½³å¯ç”¨è®¾å¤‡ (CUDA > MPS > CPU)"""
    if torch.cuda.is_available():
        logger.info("âœ… æ£€æµ‹åˆ° CUDA è®¾å¤‡ï¼Œå°†ä½¿ç”¨ GPU åŠ é€Ÿã€‚")
        return 'cuda'
    if torch.backends.mps.is_available():
        logger.info("âœ… æ£€æµ‹åˆ° Apple Silicon (MPS) è®¾å¤‡ï¼Œå°†ä½¿ç”¨ GPU åŠ é€Ÿã€‚")
        return 'mps'
    logger.info("âš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œå°†ä½¿ç”¨ CPUã€‚")
    return 'cpu'

DEFAULT_DEVICE = get_optimal_device()

# --- æ¨¡å‹ç®¡ç†å™¨ (å•ä¾‹æ¨¡å¼ï¼Œé¿å…é‡å¤åŠ è½½) ---
class ModelManager:
    _instance = None
    _models_cache = {}

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def get_embedding_model(self, model_name: str, device: str) -> HuggingFaceEmbeddings:
        cache_key = f"{model_name}_{device}"
        if cache_key in self._models_cache:
            logger.info(f"ğŸš€ ä»ç¼“å­˜ä¸­å¤ç”¨åµŒå…¥æ¨¡å‹: {model_name} on {device}")
            return self._models_cache[cache_key]

        logger.info(f"â³ æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹: {model_name} -> è®¾å¤‡: {device}")
        start_time = time.time()
        
        model_kwargs = {'device': device}
        encode_kwargs = {'normalize_embeddings': True, 'batch_size': 128}
        
        if device != 'cpu':
            model_kwargs['torch_dtype'] = torch.float16

        embeddings = HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs=model_kwargs,
            encode_kwargs=encode_kwargs,
            show_progress=True
        )
        self._models_cache[cache_key] = embeddings
        logger.info(f"âœ… åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f} ç§’")
        return embeddings

MODEL_MANAGER = ModelManager()

# --- æ ¸å¿ƒå‡½æ•°ï¼šåˆ›å»ºæ··åˆæ£€ç´¢å™¨ ---
def create_hybrid_retriever(
    documents: List[Document],
    top_k: int = 8,
    model_name: str = "BAAI/bge-base-zh-v1.5",
    device: str = DEFAULT_DEVICE,
    bm25_weight: float = 0.5,
    faiss_weight: float = 0.5
) -> BaseRetriever:
    """
    åˆ›å»ºå¹¶è¿”å›ä¸€ä¸ªé›†æˆäº† BM25 (ç¨€ç–) å’Œ FAISS (ç¨ å¯†) çš„é«˜çº§æ··åˆæ£€ç´¢å™¨ã€‚

    Args:
        documents (List[Document]): LangChainçš„æ–‡æ¡£å¯¹è±¡åˆ—è¡¨ã€‚
        top_k (int): æ¯ä¸ªæ£€ç´¢å™¨å¸Œæœ›å¬å›çš„æ–‡æ¡£æ•°é‡ã€‚
        model_name (str): ç”¨äºç¨ å¯†æ£€ç´¢çš„åµŒå…¥æ¨¡å‹ã€‚
        device (str): è¿è¡ŒåµŒå…¥æ¨¡å‹çš„è®¾å¤‡ã€‚
        bm25_weight (float): BM25 åœ¨RRFèåˆä¸­çš„æƒé‡ã€‚
        faiss_weight (float): FAISS åœ¨RRFèåˆä¸­çš„æƒé‡ã€‚

    Returns:
        BaseRetriever: ä¸€ä¸ªé…ç½®å¥½çš„EnsembleRetrieverå®ä¾‹ï¼Œå¯ç›´æ¥ç”¨äºLangChainé“¾ã€‚
    """
    if not documents:
        raise ValueError("è¾“å…¥çš„æ–‡æ¡£åˆ—è¡¨ä¸èƒ½ä¸ºç©ºã€‚")

    logger.info("--- å¼€å§‹åˆ›å»ºæ··åˆæ£€ç´¢å™¨ (BM25 + FAISS) ---")
    start_time = time.time()

    # 1. åˆ›å»º BM25 (ç¨€ç–) æ£€ç´¢å™¨
    # ä½¿ç”¨LangChainå®˜æ–¹ç»„ä»¶ï¼Œå®ƒç»è¿‡ä¼˜åŒ–å¹¶ä¸”å…¼å®¹æ•´ä¸ªç”Ÿæ€
    logger.info(f"æ­£åœ¨åˆå§‹åŒ– BM25Retriever (k={top_k})...")
    bm25_retriever = BM25Retriever.from_documents(documents)
    bm25_retriever.k = top_k

    # 2. åˆ›å»º FAISS (ç¨ å¯†) æ£€ç´¢å™¨
    logger.info(f"æ­£åœ¨åˆå§‹åŒ– FAISS vectorstore and retriever (k={top_k})...")
    embeddings = MODEL_MANAGER.get_embedding_model(model_name, device)
    
    vectorstore = FAISS.from_documents(documents=documents, embedding=embeddings)
    faiss_retriever = vectorstore.as_retriever(search_kwargs={"k": top_k})

    # 3. åˆ›å»º Ensemble (æ··åˆ) æ£€ç´¢å™¨
    # ä½¿ç”¨RRFç®—æ³•è¿›è¡Œæ™ºèƒ½èåˆï¼Œæ— éœ€æ‰‹åŠ¨å¤„ç†åˆ†æ•°
    logger.info(f"æ­£åœ¨åˆå§‹åŒ– EnsembleRetriever (weights: BM25={bm25_weight}, FAISS={faiss_weight})...")
    ensemble_retriever = EnsembleRetriever(
        retrievers=[bm25_retriever, faiss_retriever],
        weights=[bm25_weight, faiss_weight]
    )
    
    logger.info(f"âœ… æ··åˆæ£€ç´¢å™¨åˆ›å»ºæˆåŠŸï¼æ€»è€—æ—¶: {time.time() - start_time:.2f} ç§’")
    return ensemble_retriever
