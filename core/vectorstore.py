# PaperMind/core/vectorstore.py

"""
é«˜æ€§èƒ½å‘é‡æ•°æ®åº“æ¨¡å— (v2.2 - FAISS Optimized)

æ ¸å¿ƒä¼˜åŒ–ç‚¹ï¼š
1.  **FAISS ä¼˜å…ˆ**: é»˜è®¤ä½¿ç”¨FAISSï¼Œå®ƒåœ¨çº¯CPUå’ŒGPUç¯å¢ƒä¸‹é€šå¸¸æ¯”Chromaæ›´å¿«ã€‚
2.  **GPUä¼˜å…ˆä¸åŠç²¾åº¦æµ®ç‚¹ (FP16)**: è‡ªåŠ¨æ£€æµ‹CUDA/MPSå¹¶ä½¿ç”¨FP16åŠ é€Ÿã€‚
3.  **æ™ºèƒ½æ¨¡å‹ç®¡ç†**: é€šè¿‡å•ä¾‹æ¨¡å¼ç¼“å­˜å·²åŠ è½½çš„æ¨¡å‹ï¼Œé¿å…é‡å¤åŠ è½½ã€‚
4.  **ä»£ç ç®€æ´**: æä¾›æ¸…æ™°ã€ç›´æ¥çš„å‡½æ•°æ¥å£ï¼Œæ˜“äºè°ƒç”¨å’Œç»´æŠ¤ã€‚
"""
import os
import time
import logging
from typing import List

# ç¡®ä¿åœ¨å¯¼å…¥torchä¹‹å‰è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œä»¥é¿å…æ½œåœ¨çš„å†²çª
os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'

from langchain.schema.document import Document
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.schema.retriever import BaseRetriever
import torch

# --- å…¨å±€é…ç½®ä¸è®¾å¤‡æ£€æµ‹ ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def get_optimal_device() -> str:
    if torch.cuda.is_available():
        logger.info("âœ… æ£€æµ‹åˆ° CUDA è®¾å¤‡ï¼Œå°†ä½¿ç”¨ GPU åŠ é€Ÿã€‚")
        return 'cuda'
    if torch.backends.mps.is_available():
        logger.info("âœ… æ£€æµ‹åˆ° Apple Silicon (MPS) è®¾å¤‡ï¼Œå°†ä½¿ç”¨ GPU åŠ é€Ÿã€‚")
        return 'mps'
    logger.info("âš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œå°†ä½¿ç”¨ CPUã€‚")
    return 'cpu'

DEFAULT_DEVICE = get_optimal_device()

# --- æ¨¡å‹ç®¡ç†å™¨ ---
class ModelManager:
    _instance = None
    _models_cache = {}

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def get_embedding_model(self, model_name: str, device: str) -> HuggingFaceEmbeddings:
        cache_key = f"{model_name}_{device}"
        if cache_key in self._models_cache:
            logger.info(f"ğŸš€ ä»ç¼“å­˜ä¸­å¤ç”¨åµŒå…¥æ¨¡å‹: {model_name} on {device}")
            return self._models_cache[cache_key]

        logger.info(f"â³ æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹: {model_name} -> è®¾å¤‡: {device}")
        start_time = time.time()
        
        model_kwargs = {'device': device}
        encode_kwargs = {'normalize_embeddings': True, 'batch_size': 128} # å¢å¤§batch_sizeä»¥åˆ©ç”¨GPU
        
        if device != 'cpu':
            model_kwargs['torch_dtype'] = torch.float16

        embeddings = HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs=model_kwargs,
            encode_kwargs=encode_kwargs,
            show_progress=True # æ˜¾ç¤ºè¿›åº¦æ¡
        )
        self._models_cache[cache_key] = embeddings
        logger.info(f"âœ… åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f} ç§’")
        return embeddings

MODEL_MANAGER = ModelManager()

def create_vectorstore(chunks: List[str],
                       model_name: str = "BAAI/bge-base-zh-v1.5",
                       device: str = DEFAULT_DEVICE) -> FAISS:
    if not chunks:
        raise ValueError("è¾“å…¥æ–‡æœ¬å—åˆ—è¡¨ä¸èƒ½ä¸ºç©ºã€‚")

    logger.info(f"--- å¼€å§‹åˆ›å»ºFAISSå‘é‡æ•°æ®åº“ ---")
    start_time = time.time()
    
    embeddings = MODEL_MANAGER.get_embedding_model(model_name, device)
    
    # FAISS.from_texts å†…éƒ¨å·²åšé«˜æ•ˆä¼˜åŒ–
    vectorstore = FAISS.from_texts(texts=chunks, embedding=embeddings)
    
    logger.info(f"âœ… FAISSå‘é‡æ•°æ®åº“åˆ›å»ºæˆåŠŸï¼æ€»è€—æ—¶: {time.time() - start_time:.2f} ç§’")
    return vectorstore

def get_retriever(vectorstore: FAISS, top_k: int = 4) -> BaseRetriever:
    logger.info(f"åˆ›å»ºæ£€ç´¢å™¨ï¼Œå°†è¿”å› top {top_k} ä¸ªç›¸å…³ç»“æœã€‚")
    return vectorstore.as_retriever(search_kwargs={"k": top_k})

# --- å•å…ƒæµ‹è¯• ---
if __name__ == '__main__':
    from splitter import split_text

    sample_paper_text = ("Abstract: This paper introduces a novel method...\n" * 5 + 
                         "1. Introduction: The field of NLP...\n" * 10 +
                         "2. Methodology: We employed a technique...\n" * 15)
    
    logger.info("--- æ­¥éª¤1: åˆ‡åˆ†æ–‡æœ¬ ---")
    test_chunks = split_text(sample_paper_text, chunk_size=300)
    logger.info(f"âœ… ç”Ÿæˆ {len(test_chunks)} ä¸ªæ–‡æœ¬å—\n")

    logger.info(f"--- æ­¥éª¤2: åˆ›å»ºå‘é‡æ•°æ®åº“ (ä½¿ç”¨è®¾å¤‡: {DEFAULT_DEVICE}) ---")
    try:
        vs = create_vectorstore(test_chunks, model_name="BAAI/bge-small-zh-v1.5")
        
        logger.info("\n--- æ­¥éª¤3: æµ‹è¯•æ£€ç´¢ ---")
        retriever = get_retriever(vs, top_k=2)
        query = "What is the novel method?"
        results = retriever.invoke(query)
        
        print("\næ£€ç´¢ç»“æœ:")
        for doc in results:
            print(f"- {doc.page_content[:150]}...")
            
        logger.info("\nâœ… å‘é‡å­˜å‚¨æ¨¡å—æµ‹è¯•æˆåŠŸï¼")
            
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•è¿è¡Œå¤±è´¥: {e}")
        logger.info("è¯·ç¡®ä¿å·²å®‰è£…å¿…è¦çš„ä¾èµ–ï¼špip install faiss-cpu sentence-transformers")