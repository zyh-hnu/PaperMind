#è´Ÿè´£æž„å»ºå’Œç®¡ç†LangChainé“¾
"""
- **`chain.py` - å¯¹è¯é“¾æž„å»ºå™¨**
    - **èŒè´£**: ç»„è£…LangChainçš„å„ä¸ªç»„ä»¶ï¼Œæž„å»ºæœ€ç»ˆçš„é—®ç­”é“¾æˆ–å¯¹è¯é“¾ã€‚
    - **æ ¸å¿ƒå‡½æ•°**: `get_conversation_chain(retriever)`ï¼ŒæŽ¥æ”¶ä¸€ä¸ªæ£€ç´¢å™¨ï¼Œé…ç½®å¥½Promptæ¨¡æ¿ã€LLMï¼ˆæ™ºè°±AIï¼‰ã€ä»¥åŠå¯¹è¯è®°å¿†(Memory)ï¼Œæœ€ç»ˆè¿”å›žä¸€ä¸ªå¯ä»¥å¤„ç†ç”¨æˆ·æé—®çš„`chain`å¯¹è±¡ã€‚
"""
# PaperMind/core/chain.py
"""
è¿™ä¸ªæ¨¡å—è´Ÿè´£æž„å»ºå’Œç®¡ç†LangChainé“¾ã€‚
å®ƒå°†æ£€ç´¢å™¨å’ŒGoogle Geminiè¯­è¨€æ¨¡åž‹è¿žæŽ¥èµ·æ¥ï¼Œå½¢æˆä¸€ä¸ªå®Œæ•´çš„RAGé—®ç­”é“¾ã€‚

æ€§èƒ½ä¼˜åŒ–è¦ç‚¹ï¼ˆä½Žå»¶è¿Ÿï¼‰ï¼š
- ä½¿ç”¨ Gemini Flash ç³»åˆ—ï¼ˆé»˜è®¤ gemini-2.5-flashï¼‰
- é™åˆ¶æ£€ç´¢æ–‡æ¡£æ•°é‡ä¸Žæ€»ä¸Šä¸‹æ–‡å­—ç¬¦æ•°ï¼ˆå‡å°‘æç¤ºè¯ä½“ç§¯ï¼‰
- æˆªæ–­å•æ–‡æ¡£ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆé¿å…é•¿æ®µæ–‡æœ¬æ‹–æ…¢æŽ¨ç†ï¼‰
- é™åˆ¶è¾“å‡ºæœ€å¤§ token æ•°ï¼ˆæ›´å¿«å‡ºç»“æžœï¼‰
- å¤ç”¨å·²åˆ›å»ºçš„ LLM å®žä¾‹ï¼ˆé¿å…é‡å¤åˆå§‹åŒ–ï¼‰
"""

import os
import logging
from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain.schema.retriever import BaseRetriever
from langchain_google_genai import ChatGoogleGenerativeAI


# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# --- ç½‘ç»œä»£ç†è®¾ç½® ---
# å°†ä»£ç†è®¾ç½®æ”¾åœ¨å…¨å±€ï¼Œä»¥ä¾¿æ•´ä¸ªPythonä¼šè¯éƒ½å¯ä½¿ç”¨
PROXY_URL = "http://127.0.0.1:10808" # âœ… è¯·ç¡®ä¿è¿™æ˜¯ä½ æ­£ç¡®çš„ä»£ç†ç«¯å£
if PROXY_URL:
    os.environ["HTTP_PROXY"] = PROXY_URL
    os.environ["HTTPS_PROXY"] = PROXY_URL
    logger.info(f"å·²è®¾ç½®ä»£ç†: {PROXY_URL}")
    
_LLM_CACHE = {}

def _get_cached_llm(model_name: str, api_key: str, temperature: float, max_output_tokens: int) -> ChatGoogleGenerativeAI:
    """åŸºäºŽæ¨¡åž‹é…ç½®å¤ç”¨ LLM å®žä¾‹ä»¥é¿å…é‡å¤åˆå§‹åŒ–æˆæœ¬ã€‚"""
    cache_key = (model_name, temperature, max_output_tokens)
    if cache_key in _LLM_CACHE:
        return _LLM_CACHE[cache_key]
    

    llm = ChatGoogleGenerativeAI(
        model=model_name,
        google_api_key=api_key,
        temperature=temperature,
        max_output_tokens=max_output_tokens
    )
    _LLM_CACHE[cache_key] = llm
    return llm

def get_conversation_chain(
    retriever: BaseRetriever,
    *,
    model_name: str = "gemini-2.5-flash",
    temperature: float = 1.0,
    max_output_tokens: int = 6000,
    max_docs: int = 5,
    per_doc_char_limit: int = 1000,
    total_context_char_limit: int = 4000,
):
    """
    æž„å»ºä¸€ä¸ªå®Œæ•´çš„ã€ä½¿ç”¨Google Geminiçš„RAGå¯¹è¯é“¾ï¼ˆä½Žå»¶è¿Ÿä¼˜åŒ–ï¼‰ã€‚

    å‚æ•°:
        retriever: æ–‡æ¡£æ£€ç´¢å™¨
        model_name: Gemini æ¨¡åž‹åï¼ˆé»˜è®¤ gemini-2.5-flashï¼‰
        temperature: é‡‡æ ·æ¸©åº¦
        max_output_tokens: å›žç­”çš„æœ€å¤§è¾“å‡º token æ•°ï¼ˆè¶Šå°è¶Šå¿«ï¼‰
        max_docs: å‚ä¸Žä¸Šä¸‹æ–‡æ‹¼æŽ¥çš„æ–‡æ¡£æ•°é‡ä¸Šé™
        per_doc_char_limit: å•æ–‡æ¡£æ‹¼æŽ¥åˆ°æç¤ºè¯çš„å­—ç¬¦ä¸Šé™
        total_context_char_limit: æ‰€æœ‰ä¸Šä¸‹æ–‡æ€»å­—ç¬¦ä¸Šé™
    """
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        raise ValueError("æœªèƒ½åŠ è½½ GOOGLE_API_KEYã€‚è¯·ç¡®ä¿ä½ çš„.envæ–‡ä»¶åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸”é…ç½®æ­£ç¡®ã€‚")

    llm = _get_cached_llm(
        model_name=model_name,
        api_key=api_key,
        temperature=temperature,
        max_output_tokens=max_output_tokens,
    )

    prompt_template = (
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIå­¦æœ¯è®ºæ–‡é˜…è¯»åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹é¢æä¾›çš„è®ºæ–‡ç‰‡æ®µä½œä¸ºå”¯ä¸€çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç”¨æ¸…æ™°ã€ç®€æ´çš„ä¸­æ–‡å›žç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"
        "å¦‚æžœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰è¶³å¤Ÿçš„ä¿¡æ¯æ¥å›žç­”é—®é¢˜ï¼Œè¯·ç›´æŽ¥è¯´â€œæ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ— æ³•å›žç­”è¿™ä¸ªé—®é¢˜ã€‚â€ ä¸è¦ç¼–é€ ä»»ä½•ä¸Šä¸‹æ–‡ä¹‹å¤–çš„ä¿¡æ¯ã€‚\n\n"
        "ðŸ“Œ ä¸Šä¸‹æ–‡:\n{context}\n\n"
        "â“ é—®é¢˜:\n{question}\n\n"
        "ðŸ¤– å›žç­”:"
    )
    prompt = PromptTemplate.from_template(prompt_template)

    def _truncate_text(text: str, limit: int) -> str:
        if limit <= 0 or len(text) <= limit:
            return text
        return text[:limit]

    def format_docs(docs):
        # 1) é™åˆ¶æ–‡æ¡£æ•°é‡
        selected = docs[: max_docs]
        # 2) æˆªæ–­æ¯ä¸ªæ–‡æ¡£
        truncated = [_truncate_text(d.page_content, per_doc_char_limit) for d in selected]
        # 3) åˆå¹¶å¹¶é™åˆ¶æ€»ä¸Šä¸‹æ–‡
        merged = "\n\n".join(truncated)
        if total_context_char_limit and len(merged) > total_context_char_limit:
            merged = merged[: total_context_char_limit]
        return merged

    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    return rag_chain
