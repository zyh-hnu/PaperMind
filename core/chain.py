# PaperMind/core/chain.py

"""
æ„å»ºRAGå¯¹è¯é“¾ (v4.0 - Rerank Integrated)

æ ¸å¿ƒæµç¨‹:
1.  **HyDEæŸ¥è¯¢å¢å¼º**: æ¥æ”¶ç”¨æˆ·é—®é¢˜ï¼Œç”Ÿæˆå‡è®¾æ€§æ–‡æ¡£ä»¥ä¼˜åŒ–æŸ¥è¯¢ã€‚
2.  **æ··åˆæ£€ç´¢ (å¬å›)**: ä½¿ç”¨å¢å¼ºæŸ¥è¯¢ä»EnsembleRetrieverä¸­â€œå¬å›â€ä¸€æ‰¹å€™é€‰æ–‡æ¡£ï¼ˆæ•°é‡å¯ä»¥å¤šä¸€äº›ï¼Œæ¯”å¦‚15-20ä¸ªï¼‰ã€‚
3.  **é‡æ’åº (ç²¾æ’)**: ä½¿ç”¨BGE-Rerankeræ¨¡å‹å¯¹å€™é€‰æ–‡æ¡£è¿›è¡Œç²¾å‡†çš„ç›¸å…³æ€§æ‰“åˆ†å’Œæ’åºã€‚
4.  **ä¸Šä¸‹æ–‡é€‰æ‹©ä¸æ ¼å¼åŒ–**: é€‰æ‹©é‡æ’åºåå¾—åˆ†æœ€é«˜çš„Nä¸ªæ–‡æ¡£ï¼Œæ‹¼æ¥æˆæœ€ç»ˆçš„ä¸Šä¸‹æ–‡ã€‚
5.  **æœ€ç»ˆç”Ÿæˆ**: å°†é«˜è´¨é‡çš„ä¸Šä¸‹æ–‡å’ŒåŸå§‹é—®é¢˜é€å…¥LLMï¼Œç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚
"""

import os
import logging
from typing import List, Dict
import torch

from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain.schema.retriever import BaseRetriever
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.documents import Document
from sentence_transformers.cross_encoder import CrossEncoder
from langchain_core.runnables import RunnableLambda

# --- å…¨å±€é…ç½®ä¸æ—¥å¿— ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- ä»£ç†è®¾ç½® ---
PROXY_URL = "http://127.0.0.1:10808"
if PROXY_URL:
    os.environ["HTTP_PROXY"] = PROXY_URL
    os.environ["HTTPS_PROXY"] = PROXY_URL
    logger.info(f"å·²è®¾ç½®ä»£ç†: {PROXY_URL}")

# --- è®¾å¤‡è‡ªåŠ¨æ£€æµ‹ ---
def get_optimal_device() -> str:
    if torch.cuda.is_available():
        return 'cuda'
    if torch.backends.mps.is_available():
        return 'mps'
    return 'cpu'

DEFAULT_DEVICE = get_optimal_device()

# --- æ¨¡å‹ç¼“å­˜ç®¡ç†å™¨ ---
_LLM_CACHE = {}
_RERANKER_CACHE = {}

def _get_cached_llm(model_name: str, api_key: str, temperature: float) -> ChatGoogleGenerativeAI:
    """åŸºäºé…ç½®å¤ç”¨LLMå®ä¾‹"""
    cache_key = (model_name, temperature)
    if cache_key not in _LLM_CACHE:
        logger.info(f"æ­£åœ¨åˆå§‹åŒ–æ–°çš„LLMå®ä¾‹: model={model_name}, temp={temperature}")
        _LLM_CACHE[cache_key] = ChatGoogleGenerativeAI(
            model=model_name,
            google_api_key=api_key,
            temperature=temperature,
        )
    return _LLM_CACHE[cache_key]

def _get_cached_reranker(model_name: str, device: str) -> CrossEncoder:
    """åŠ è½½å¹¶ç¼“å­˜Rerankeræ¨¡å‹"""
    cache_key = (model_name, device)
    if cache_key not in _RERANKER_CACHE:
        logger.info(f"â³ æ­£åœ¨åŠ è½½Rerankeræ¨¡å‹: {model_name} -> è®¾å¤‡: {device}")
        _RERANKER_CACHE[cache_key] = CrossEncoder(model_name, device=device, max_length=1024)
        logger.info("âœ… Rerankeræ¨¡å‹åŠ è½½å®Œæˆã€‚")
    return _RERANKER_CACHE[cache_key]


# --- æ ¸å¿ƒå‡½æ•°ï¼šæ„å»ºå¯¹è¯é“¾ ---
def get_conversation_chain(
    retriever: BaseRetriever,
    *,
    # LLM & RAG ç›¸å…³å‚æ•°
    llm_model_name: str = "gemini-2.5-flash",
    temperature: float = 1.0,
    rerank_model_name: str = r"D:\Project2025\PaperMind\models\bge-reranker-base", 
    rerank_top_n: int = 5, # æœ€ç»ˆé€‰æ‹©é‡æ’åæœ€å¥½çš„Nä¸ªæ–‡æ¡£
    total_context_char_limit: int = 8000
):
    """
    æ„å»ºä¸€ä¸ªé›†æˆäº† HyDEã€æ··åˆæ£€ç´¢ å’Œ Reranker çš„é«˜çº§RAGå¯¹è¯é“¾ã€‚
    """
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        raise ValueError("æœªèƒ½åŠ è½½ GOOGLE_API_KEYã€‚è¯·ç¡®ä¿.envæ–‡ä»¶é…ç½®æ­£ç¡®ã€‚")


    def format_context(docs: List[Document]) -> str:
        parts = []
        for i, doc in enumerate(docs[:rerank_top_n], 1):
            source_info = f"[æ¥æº: {doc.metadata.get('source', 'æœªçŸ¥')} | å¾—åˆ†: {doc.metadata.get('score', 'N/A'):.4f}]"
            parts.append(f"ã€æ®µè½ {i}ã€‘{source_info}\n{doc.page_content}")
        return "\n\n" + "\n\n".join(parts)

    # 1. è·å–LLMå’ŒRerankeræ¨¡å‹å®ä¾‹
    llm = _get_cached_llm(llm_model_name, api_key, temperature)
    reranker = _get_cached_reranker(rerank_model_name, DEFAULT_DEVICE)

    # 2. HyDE (Hypothetical Document Embeddings) é“¾
    hyde_template = """
    ä½ æ˜¯ä¸€ä½AIå­¦æœ¯ç ”ç©¶åŠ©ç†ã€‚è¯·æ ¹æ®ç”¨æˆ·æå‡ºçš„ä»¥ä¸‹é—®é¢˜ï¼Œç”Ÿæˆä¸€ä¸ªç®€æ´ã€ä¸“ä¸šçš„æ®µè½ï¼Œå°±å¥½åƒå®ƒæ˜¯ä»ä¸€ç¯‡ç›¸å…³å­¦æœ¯è®ºæ–‡ä¸­æ‘˜å½•å‡ºæ¥çš„ç­”æ¡ˆä¸€æ ·ã€‚
    è¿™ä¸ªæ®µè½å°†ç”¨äºè¿›è¡Œå‘é‡æ£€ç´¢ï¼Œå› æ­¤å®ƒåº”è¯¥åŒ…å«ä¸é—®é¢˜æ ¸å¿ƒæ¦‚å¿µç›¸å…³çš„å…³é”®è¯ã€æœ¯è¯­å’Œå…³é”®ä¿¡æ¯ã€‚
    ç”¨æˆ·é—®é¢˜: "{question}"
    ç”Ÿæˆçš„å‡è®¾æ€§å­¦æœ¯æ®µè½:"""
    hyde_prompt = PromptTemplate.from_template(hyde_template)
    hyde_chain = hyde_prompt | llm | StrOutputParser()

    # 3. RAG æœ€ç»ˆé—®ç­”æç¤º
    rag_template = """
    ä½ æ˜¯ä¸€ä½é¡¶çº§çš„AIå­¦æœ¯è®ºæ–‡é˜…è¯»åŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼æ ¹æ®ä¸‹é¢æä¾›çš„â€œä¸Šä¸‹æ–‡ä¿¡æ¯â€ï¼Œç”¨æ¸…æ™°ã€ä¸“ä¸šã€ç®€æ´çš„ä¸­æ–‡å›ç­”ç”¨æˆ·çš„â€œé—®é¢˜â€ã€‚
    å›ç­”è§„åˆ™:
    1. ä½ çš„å›ç­”å¿…é¡»å®Œå…¨åŸºäºæ‰€æä¾›çš„ä¸Šä¸‹æ–‡ï¼Œç¦æ­¢åˆ©ç”¨ä½ çš„ä»»ä½•å…ˆéªŒçŸ¥è¯†ã€‚
    2. å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰è¶³å¤Ÿçš„ä¿¡æ¯æ¥å›ç­”é—®é¢˜ï¼Œå¿…é¡»ç›´æ¥å›å¤ï¼šâ€œæ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚â€
    3. ä¿æŒç­”æ¡ˆçš„å®¢è§‚æ€§å’Œå‡†ç¡®æ€§ã€‚
    4. ä¸¥ç¦ä»»ä½•å½¢å¼çš„çŒœæµ‹ã€æ¨æ–­æˆ–ç¼–é€ ã€‚å¦‚æœä½ ä¸ç¡®å®šï¼Œå°±è¯´â€œæ— æ³•å›ç­”â€ã€‚

    --- ä¸Šä¸‹æ–‡ä¿¡æ¯ ---
    {context}

    --- é—®é¢˜ ---
    {question}

    --- ä½ çš„å›ç­” ---"""
    rag_prompt = PromptTemplate.from_template(rag_template)

    # 4. å®šä¹‰é‡æ’åºå‡½æ•°
    def rerank_documents(inputs: Dict) -> List[Document]:
        """å¯¹æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨è¿›è¡Œé‡æ’åº"""
        question = inputs["question"]
        docs = inputs["documents"]
        
        if not docs:
            return []
            
        logger.info(f"ğŸ” Rerankeræ­£åœ¨å¯¹ {len(docs)} ä¸ªæ–‡æ¡£è¿›è¡Œç²¾æ’...")
        
        # åˆ›å»º [æŸ¥è¯¢, æ–‡æ¡£] å¯¹
        pairs = [(question, doc.page_content) for doc in docs]
        
        # è®¡ç®—ç›¸å…³æ€§å¾—åˆ†
        scores = reranker.predict(pairs, show_progress_bar=False)
        
        # å°†å¾—åˆ†ä¸æ–‡æ¡£æ‰“åŒ…å¹¶æ’åº
        scored_docs = sorted(zip(scores, docs), key=lambda x: x[0], reverse=True)
        
        # åªè¿”å›é‡æ’åºåçš„æ–‡æ¡£åˆ—è¡¨
        reranked_docs = [doc for score, doc in scored_docs]
        
        logger.info("âœ… ç²¾æ’å®Œæˆï¼")
        return reranked_docs

    def retrieve_docs(query: str) -> List[Document]:
        return retriever.invoke(query)

    # 5. æ„å»ºå®Œæ•´çš„LCELæ‰§è¡Œé“¾
    rag_chain = (
        {
            # ç¬¬ä¸€éƒ¨åˆ†ï¼šè·å–ä¸Šä¸‹æ–‡
            "context": {
                "documents": (lambda x: x['question']) | hyde_chain | RunnableLambda(retrieve_docs),
                "question": lambda x: x['question']
            }
            | RunnableLambda(rerank_documents) # æ‰§è¡Œé‡æ’åº
            | (lambda docs: "\n\n".join(
            f"ã€æ®µè½ {i+1}ã€‘[æ¥æº: {doc.metadata.get('source', 'æœªçŸ¥')}]\n{doc.page_content}"
            for i, doc in enumerate(docs[:rerank_top_n])
            ))
            ,
            # ç¬¬äºŒéƒ¨åˆ†ï¼šä¼ é€’åŸå§‹é—®é¢˜
            "question": lambda x: x['question']
        }
        | rag_prompt
        | llm
        | StrOutputParser()
    )
    
    # å°è£…åœ¨ä¸€ä¸ªRunnablePassthroughä¸­ï¼Œä½¿å…¶æ¥æ”¶ä¸€ä¸ªç®€å•çš„å­—ç¬¦ä¸²è¾“å…¥
    #final_chain = RunnablePassthrough.assign(question=lambda x: x) | rag_chain

    logger.info("âœ… Rerank å¢å¼ºçš„RAGå¯¹è¯é“¾æ„å»ºæˆåŠŸï¼")
    return rag_chain